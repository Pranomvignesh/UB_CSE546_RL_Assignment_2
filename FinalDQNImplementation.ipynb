{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a94133b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, observation_space,action_space, learningRate):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(observation_space, 256)\n",
    "        self.layer_2 = nn.Linear(256, 256)\n",
    "        self.layer_3 = nn.Linear(256, action_space)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learningRate)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        return self.layer_3(x)\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, size):\n",
    "        self.memory = deque([],maxlen=size)\n",
    "\n",
    "    def add(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batchSize):\n",
    "        return random.sample(self.memory,batchSize)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, env, hyperparams, nnModel):\n",
    "        self.env = env\n",
    "        self.epsilon = hyperparams.epsilon\n",
    "        self.epsilonMax = self.epsilon\n",
    "        self.epsilonMin = hyperparams.epsilonMin\n",
    "        self.epsilonDecay = hyperparams.epsilonDecay\n",
    "        self.discountFactor = hyperparams.discountFactor\n",
    "        self.updateFrequency = hyperparams.targetNetworkUpdateFrequency\n",
    "        self.batchSize = hyperparams.batchSize\n",
    "        self.episodes = hyperparams.episodes\n",
    "        self.action_space = env.action_space.n\n",
    "        self.learningRate = hyperparams.learningRate\n",
    "        self.observation_space = env.observation_space.shape[0]\n",
    "        self.memory = ReplayMemory(hyperparams.memorySize)\n",
    "        self.policyNetwork = nnModel(self.observation_space, self.action_space, self.learningRate)\n",
    "        self.targetNetwork = nnModel(self.observation_space, self.action_space, self.learningRate)\n",
    "        self.targetNetwork.load_state_dict(self.policyNetwork.state_dict())\n",
    "        self.iterations = 0\n",
    "\n",
    "    def getAction(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        state = torch.tensor(state).float().detach()\n",
    "        state = state.to(device)\n",
    "        state = state.unsqueeze(0)\n",
    "        qValues = self.policyNetwork(state)\n",
    "        return torch.argmax(qValues).item()\n",
    "\n",
    "    def optimize(self):\n",
    "        batchSize = self.batchSize\n",
    "        if len(self.memory) > batchSize:\n",
    "            minibatch = np.array(self.memory.sample(batchSize))\n",
    "            states = minibatch[:, 0].tolist()\n",
    "            actions = minibatch[:, 1].tolist()\n",
    "            rewards = minibatch[:, 2].tolist()\n",
    "            nextStates = minibatch[:, 3].tolist()\n",
    "            dones = minibatch[:, 4].tolist()\n",
    "\n",
    "            states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "            actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "            nextStates = torch.tensor(\n",
    "                nextStates, dtype=torch.float32).to(device)\n",
    "            dones = torch.tensor(dones, dtype=torch.bool).to(device)\n",
    "            indices = np.arange(batchSize, dtype=np.int64)\n",
    "\n",
    "            qValues = self.policyNetwork(states)\n",
    "            qDotValues = None\n",
    "            with torch.no_grad():\n",
    "                qDotValues = self.targetNetwork(nextStates)\n",
    "\n",
    "            predictedValues = qValues[indices, actions]\n",
    "            predictedQDotValues = torch.max(qDotValues, dim=1)[0]\n",
    "\n",
    "            targetValues = rewards + self.discountFactor * predictedQDotValues * dones\n",
    "\n",
    "            loss = self.policyNetwork.loss(targetValues, predictedValues)\n",
    "            self.policyNetwork.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.policyNetwork.optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        env = self.env\n",
    "        observation_space = self.observation_space\n",
    "        bestReward = 0\n",
    "        bestAverageReward = 0\n",
    "        rewards = []\n",
    "        averageRewards = []\n",
    "        for i in range(1, self.episodes):\n",
    "            state, info = env.reset()\n",
    "            state = np.reshape(state, [1, observation_space])\n",
    "            totalRewardPerEpisode = 0\n",
    "            steps = 0\n",
    "            while True:\n",
    "                action = self.getAction(state)\n",
    "                nextState, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                nextState = np.reshape(nextState, [1, observation_space])\n",
    "                self.memory.add((state[0], action, reward, nextState[0], 1 - done))\n",
    "                self.optimize()\n",
    "                state = nextState\n",
    "                totalRewardPerEpisode += reward\n",
    "                \n",
    "                diff = self.epsilonMax - self.epsilonMin\n",
    "                decayed_epsilon = self.epsilonMin + diff * \\\n",
    "                    np.exp((-1 * self.iterations) / self.epsilonDecay)\n",
    "                self.iterations += 1\n",
    "                self.epsilon = max(self.epsilonMin, decayed_epsilon)\n",
    "\n",
    "                steps += 1\n",
    "                if steps % 10 == 0:\n",
    "                    self.targetNetwork.load_state_dict(\n",
    "                        self.policyNetwork.state_dict())\n",
    "\n",
    "                if done:\n",
    "                    rewards.append(totalRewardPerEpisode)\n",
    "                    if totalRewardPerEpisode > bestReward:\n",
    "                        bestReward = totalRewardPerEpisode\n",
    "\n",
    "                    averageReward = np.mean(np.array(rewards)[-100:])\n",
    "                    if averageReward > bestAverageReward:\n",
    "                        bestAverageReward = averageReward\n",
    "                    print('-'*80)\n",
    "                    print(\n",
    "                        f\"\\nEpisode {i} \\\n",
    "                          \\nAverage Reward of last 100 {averageReward} \\\n",
    "                          \\nBest Average Reward of last 100 {bestAverageReward} \\\n",
    "                          \\nBest Reward {bestReward} \\\n",
    "                          \\nCurrent Reward {totalRewardPerEpisode} \\\n",
    "                          \\nEpsilon {self.epsilon}\\n\"\n",
    "                    )\n",
    "                    averageRewards.append(averageReward)\n",
    "\n",
    "                    break\n",
    "\n",
    "                \n",
    "\n",
    "        plt.plot(averageRewards)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "Hyperparams = namedtuple('Hyperparams', (\n",
    "    'epsilon',\n",
    "    'epsilonMin',\n",
    "    'epsilonDecay',\n",
    "    'learningRate',\n",
    "    'batchSize',\n",
    "    'discountFactor',\n",
    "    'targetNetworkUpdateFrequency',\n",
    "    'episodes',\n",
    "    'memorySize'\n",
    "))\n",
    "\n",
    "dqnCartPole = DQN(\n",
    "    env=gym.make('CartPole-v1'),\n",
    "    hyperparams=Hyperparams(\n",
    "        epsilon=0.99,\n",
    "        epsilonMin=0.001,\n",
    "        epsilonDecay=10000,\n",
    "        memorySize=10000,\n",
    "        learningRate=1e-4,\n",
    "        batchSize=128,\n",
    "        discountFactor=0.99,\n",
    "        targetNetworkUpdateFrequency=20,\n",
    "        episodes=700\n",
    "    ),\n",
    "    nnModel=NeuralNetwork\n",
    ")\n",
    "\n",
    "# dqnCartPole.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31206d54",
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "box2D is not installed, run `pip install gym[box2d]`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/gym/envs/box2d/bipedal_walker.py:14\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         circleShape,\n\u001b[1;32m     17\u001b[0m         contactListener,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         revoluteJointDef,\n\u001b[1;32m     22\u001b[0m     )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Box2D'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dqnLunarLander \u001b[38;5;241m=\u001b[39m DQN(\n\u001b[0;32m----> 2\u001b[0m     env\u001b[38;5;241m=\u001b[39m\u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLunarLander-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontinuous\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgravity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_wind\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwind_power\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mturbulence_power\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     10\u001b[0m     hyperparams\u001b[38;5;241m=\u001b[39mHyperparams(\n\u001b[1;32m     11\u001b[0m         epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m,\n\u001b[1;32m     12\u001b[0m         epsilonMin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m     13\u001b[0m         epsilonDecay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,\n\u001b[1;32m     14\u001b[0m         memorySize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,\n\u001b[1;32m     15\u001b[0m         learningRate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m,\n\u001b[1;32m     16\u001b[0m         batchSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     17\u001b[0m         discountFactor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m,\n\u001b[1;32m     18\u001b[0m         targetNetworkUpdateFrequency\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     19\u001b[0m         episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m700\u001b[39m\n\u001b[1;32m     20\u001b[0m     ),\n\u001b[1;32m     21\u001b[0m     nnModel\u001b[38;5;241m=\u001b[39mNeuralNetwork\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m dqnLunarLander\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/gym/envs/registration.py:581\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m spec_\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 581\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m mode \u001b[38;5;241m=\u001b[39m _kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    584\u001b[0m apply_human_rendering \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/gym/envs/registration.py:61\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name and returns an environment creation function\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    Calls the environment constructor\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:961\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1014\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:843\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:219\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/gym/envs/box2d/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbipedal_walker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BipedalWalker, BipedalWalkerHardcore\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcar_racing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CarRacing\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbox2d\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlunar_lander\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LunarLander, LunarLanderContinuous\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/gym/envs/box2d/bipedal_walker.py:24\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mBox2D\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     16\u001b[0m         circleShape,\n\u001b[1;32m     17\u001b[0m         contactListener,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m         revoluteJointDef,\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DependencyNotInstalled(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbox2D is not installed, run `pip install gym[box2d]`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpygame\u001b[39;00m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: box2D is not installed, run `pip install gym[box2d]`"
     ]
    }
   ],
   "source": [
    "dqnLunarLander = DQN(\n",
    "    env=gym.make(\n",
    "    \"LunarLander-v2\",\n",
    "    continuous = False,\n",
    "    gravity = -10.0,\n",
    "    enable_wind = False,\n",
    "    wind_power = 15.0,\n",
    "    turbulence_power = 1.5,\n",
    "    ),\n",
    "    hyperparams=Hyperparams(\n",
    "        epsilon=0.99,\n",
    "        epsilonMin=0.001,\n",
    "        epsilonDecay=10000,\n",
    "        memorySize=10000,\n",
    "        learningRate=1e-4,\n",
    "        batchSize=128,\n",
    "        discountFactor=0.99,\n",
    "        targetNetworkUpdateFrequency=20,\n",
    "        episodes=700\n",
    "    ),\n",
    "    nnModel=NeuralNetwork\n",
    ")\n",
    "dqnLunarLander.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de95123e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicyNetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConvergedModel.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mlp/lib/python3.8/site-packages/torch/nn/modules/module.py:1269\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1268\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1269\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "dqn.policyNetwork.save('ConvergedModel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eb9e0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dqn.policyNetwork,'ConvergedModel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a353657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
